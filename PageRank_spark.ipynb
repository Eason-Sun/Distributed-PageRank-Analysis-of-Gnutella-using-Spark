{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "A3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm7N8t8XMe9l",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Dataset:\n",
        "For this project, I use Python and Spark to perform some graph analysis, using a graph of the Gnutella server network.   In this graph, each node represents a server, and each directed edge represents a connection between servers in Gnutella's peer-to-peer network.  The data file `p2p-Gnutella08-adj.txt`, represents the graph as an adjacency list.   Each server (node) is identified by a unique number, and each line in the file gives the adjacency list for a single server.\n",
        "For example, this line:\n",
        "> 91\t243\t1923\t2194\n",
        "\n",
        "gives the adjacency list for server `91`.   It indicates that there are edges from server `91` to servers `243`, `1923`, and `2194`.    According to the Stanford Network Analysis Project, which collected these data, [the graph includes 6301 servers and 20777 edges](http://snap.stanford.edu/data/p2p-Gnutella08.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp06AQNNMe9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"/u/cs451/packages/spark\")\n",
        "\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ8r679DMe9n",
        "colab_type": "text"
      },
      "source": [
        "and then create a `SparkContext`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Dgeqv1Me9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "sc = SparkContext(appName=\"YourTest\", master=\"local[2]\", conf=SparkConf().set('spark.ui.port', random.randrange(4000,5000)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdrAAIRgMe9v",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "### Determine some basic properties of the Gnutella graph:\n",
        "- Number of nodes and edges in the graph.\n",
        "- Number of nodes of each outdegree. (e.g. how many nodes have no outgoing edges, how many have one outgoing edge, and so on)\n",
        "- Number of nodes of each indegree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UZqx7mSMe9w",
        "colab_type": "code",
        "colab": {},
        "outputId": "6de3026a-0055-49d1-ecca-4667b63934ae"
      },
      "source": [
        "def num_nodes_edges():\n",
        "    \"\"\"Returns a tuple (num_nodes, num_edges)\"\"\"\n",
        "    #### Your code for Question 1.1 should go here\n",
        "    text_RDD = sc.textFile('p2p-Gnutella08-adj.txt')\n",
        "    return text_RDD.map(lambda line : (1, len(line.split())-1)) \\\n",
        "                   .reduce(lambda x, y : (x[0] + y[0], x[1] + y[1]))\n",
        "\n",
        "def out_counts():\n",
        "    \"\"\"Returns a dictionary where the keys are the outdegrees, and the \n",
        "    values are the number of nodes of the corresponding outdegree \"\"\"\n",
        "    #### Your code for Question 1.2 should go here\n",
        "    text_RDD = sc.textFile('p2p-Gnutella08-adj.txt')\n",
        "    return text_RDD.map(lambda line : (len(line.split())-1, 1)) \\\n",
        "                   .reduceByKey(lambda x, y : x + y) \\\n",
        "                   .collectAsMap()\n",
        "\n",
        "def in_counts():\n",
        "    \"\"\"Returns a dictionary where the keys are the indegrees, and the \n",
        "    values are the number of nodes of the corresponding indegree \"\"\"\n",
        "    #### Your code for Question 1.3 should go here\n",
        "    text_RDD = sc.textFile('p2p-Gnutella08-adj.txt')\n",
        "    return text_RDD.flatMap(lambda line : [(i, 1) for i in line.split()[1:]]) \\\n",
        "                   .reduceByKey(lambda x, y : x + y) \\\n",
        "                   .map(lambda pair : (pair[1], 1)) \\\n",
        "                   .reduceByKey(lambda x, y : x + y) \\\n",
        "                   .collectAsMap()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{74: 1,\n",
              " 70: 3,\n",
              " 2: 1287,\n",
              " 4: 559,\n",
              " 6: 227,\n",
              " 54: 1,\n",
              " 50: 1,\n",
              " 82: 1,\n",
              " 20: 4,\n",
              " 30: 1,\n",
              " 10: 37,\n",
              " 32: 2,\n",
              " 62: 2,\n",
              " 8: 76,\n",
              " 14: 13,\n",
              " 12: 23,\n",
              " 18: 2,\n",
              " 72: 2,\n",
              " 60: 3,\n",
              " 56: 2,\n",
              " 16: 1,\n",
              " 66: 2,\n",
              " 52: 1,\n",
              " 38: 1,\n",
              " 86: 1,\n",
              " 44: 1,\n",
              " 22: 1,\n",
              " 78: 1,\n",
              " 1: 2452,\n",
              " 59: 1,\n",
              " 49: 1,\n",
              " 7: 144,\n",
              " 5: 333,\n",
              " 11: 29,\n",
              " 71: 3,\n",
              " 81: 4,\n",
              " 19: 2,\n",
              " 13: 19,\n",
              " 67: 3,\n",
              " 21: 2,\n",
              " 57: 1,\n",
              " 47: 2,\n",
              " 3: 868,\n",
              " 9: 70,\n",
              " 15: 8,\n",
              " 69: 2,\n",
              " 25: 1,\n",
              " 33: 1,\n",
              " 31: 2,\n",
              " 77: 2,\n",
              " 73: 2,\n",
              " 51: 1,\n",
              " 85: 1,\n",
              " 83: 1,\n",
              " 87: 1,\n",
              " 61: 1,\n",
              " 27: 1,\n",
              " 35: 1,\n",
              " 63: 1,\n",
              " 79: 1,\n",
              " 55: 1,\n",
              " 41: 1,\n",
              " 91: 1,\n",
              " 23: 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq3SnuPtMe91",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Objective:\n",
        "The main objective for this project is to perform *single source personalized page rank* over the Gnutella graph. Personalized page rank is like ordinary page rank except:\n",
        "- One node in the graph is designated as the *source* node. Personalized page rank is performed with respect to that source node.\n",
        "- Personalized page rank is initialized by assigning all probability mass to the source node, and none to the other nodes. In contrast, ordinary page rank is initialized by giving all nodes the same probability mass.\n",
        "- Whenever personalized page rank makes a random jump, it jumps back to the source node. In contrast, ordinary page rank may jump to any node.\n",
        "- In personalized page rank, all probability mass lost dangling nodes is put back into the source nodes.  In ordinary page rank, lost mass is distributed evenly over all nodes.\n",
        "\n",
        "\n",
        "### Input Parameters:\n",
        "- source node id (a positive integer)\n",
        "- iteration count (a positive integer)\n",
        "- random jump factor value (a float between 0 and 1)\n",
        "\n",
        "The function performs personalized page rank, with respect to the specified source node, over the Gnutella graph, for the specified number of iterations, using Spark.\n",
        "\n",
        "The output of your function is a list of the 10 nodes with the highest personalized page rank with respect to the given source. For each of the 10 nodes, return the node's id and page rank value as a tuple. The list returned by the function looks like this: `[(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGgPcERXMe92",
        "colab_type": "code",
        "colab": {},
        "outputId": "8974ca0b-ef16-4f2e-cd20-043cfda4269d"
      },
      "source": [
        "from time import time\n",
        "from pyspark import StorageLevel\n",
        "def personalized_page_rank(source_node_id, num_iterations, jump_factor):\n",
        "    \"\"\"Returns a list of the 10 nodes with the highest page rank value along with their value, as tuples\n",
        "    [(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]\"\"\"\n",
        "    # your solution to Question 2 here\n",
        "    def construct_graph(line):\n",
        "        l = list(map(int, line.split()))\n",
        "        nid = l[0]\n",
        "        adj_list = l[1:]\n",
        "        return nid, adj_list\n",
        "    \n",
        "    def initialize_rank(line, source_node_id):\n",
        "        l = list(map(int, line.split()))\n",
        "        nid = l[0]\n",
        "        rank = 1 if nid == source_node_id else 0\n",
        "        loss = 0\n",
        "        return nid, (rank, loss)\n",
        "    \n",
        "    def spread(pair):\n",
        "        nid, (adj_list, (rank, loss)) = pair\n",
        "        if not adj_list:\n",
        "            loss += rank\n",
        "            return [(nid, (0.0, loss))]\n",
        "        num_adj_nodes = len(adj_list)\n",
        "        return list(zip(adj_list, [(rank / num_adj_nodes, 0.0)] * num_adj_nodes)) + [(nid, (0.0, 0.0))]\n",
        "    \n",
        "    def compensate_loss_jump(pair, total_loss, source_node_id, jump_factor):\n",
        "        nid, (rank, loss) = pair\n",
        "        if nid == source_node_id:\n",
        "            rank = (rank + total_loss) * (1 - jump_factor) + jump_factor\n",
        "            return nid, (rank, 0)\n",
        "        return nid, (rank * (1 - jump_factor), 0)\n",
        "    \n",
        "    text_RDD = sc.textFile('p2p-Gnutella08-adj.txt')\n",
        "    graph_RDD = text_RDD.map(lambda line : construct_graph(line))\n",
        "    rank_RDD = text_RDD.map(lambda line : initialize_rank(line, source_node_id))\n",
        "    \n",
        "    for i in range(num_iterations):                   \n",
        "        rank_RDD = graph_RDD.leftOuterJoin(rank_RDD) \\\n",
        "                            .flatMap(lambda pair : spread(pair)) \\\n",
        "                            .reduceByKey(lambda x, y : (x[0] + y[0], x[1] + y[1]))\n",
        "        total_loss = rank_RDD.aggregate(0,\n",
        "                                       (lambda acc, e : acc + e[1][1]),\n",
        "                                       (lambda acc1, acc2 : acc1 + acc2))\n",
        "        rank_RDD = rank_RDD.map(lambda e : compensate_loss_jump(e, total_loss, source_node_id, jump_factor))\n",
        "        \n",
        "    top_10_rank = rank_RDD.map(lambda e : (e[1][0], e[0])) \\\n",
        "                          .sortByKey(ascending=False) \\\n",
        "                          .map(lambda e : (e[1], e[0])) \\\n",
        "                          .take(10)\n",
        "    return top_10_rank"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(28, 0.5384424922495424),\n",
              " (1517, 0.12026767556203095),\n",
              " (1516, 0.12026368682524335),\n",
              " (846, 0.12026334287813738),\n",
              " (852, 0.008058825545098519),\n",
              " (152, 0.008057101055510536),\n",
              " (847, 0.008056064295146433),\n",
              " (850, 0.008056047096075143),\n",
              " (849, 0.008056044737059032),\n",
              " (848, 0.008056043598862838)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcAVwR2YMe94",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "#### Alternative Stopping Criterion:\n",
        "\n",
        "The above function runs PageRank for a specified number of iterations.  However, it is also common to have iterative algorithms that run until some specified termination condition is reached.\n",
        "\n",
        "For example, for page rank, suppose the $p_i(x)$ represents the probability mass assigned to node $x$ after the $i$th iteration of the algorithm.  ($p_0(x)$ is the initial probability mass of node $x$.)   We define the change of $x$'s probability mass on the $i$th iteration as $\\lvert p_i(x)-p_{i-1}(x) \\rvert$.   Then, we can iterate personalized page rank until the maximum (over all nodes) change is less than a specified threshold, i.e, until all nodes' page ranks have converged.\n",
        "\n",
        "The below function iterates until the maximum node change is less than $\\frac{0.5}{N}$, where $N$ represents the number of nodes in the graph.\n",
        "This version of the function should take only two inputs: the source node id and the random jump factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oAIipWfMe95",
        "colab_type": "code",
        "colab": {},
        "outputId": "1b4adf89-1fe7-4bb9-ee5b-b426f666e7a3"
      },
      "source": [
        "from time import time\n",
        "def personalized_page_rank_stopping_criterion(source_node_id, jump_factor):\n",
        "    \"\"\"Returns a list of the 10 nodes with the highest page rank value along with their value, as tuples\n",
        "    [(node_id_1, highest_pagerank_value), ..., (node_id_10, 10th_highest_pagerank_value)]\"\"\"\n",
        "    # your solution to Question 3 here\n",
        "    def construct_graph(line):\n",
        "        l = list(map(int, line.split()))\n",
        "        nid = l[0]\n",
        "        adj_list = l[1:]\n",
        "        return nid, adj_list\n",
        "    \n",
        "    def initialize_rank(line, source_node_id):\n",
        "        l = list(map(int, line.split()))\n",
        "        nid = l[0]\n",
        "        rank = 1 if nid == source_node_id else 0\n",
        "        loss = 0\n",
        "        return nid, (rank, loss)\n",
        "    \n",
        "    def spread(pair):\n",
        "        nid, (adj_list, (rank, loss)) = pair\n",
        "        if not adj_list:\n",
        "            loss += rank\n",
        "            return [(nid, (0, loss))]\n",
        "        num_adj_nodes = len(adj_list)\n",
        "        return list(zip(adj_list, [(rank / num_adj_nodes, 0)] * num_adj_nodes)) + [(nid, (0, 0))]\n",
        "    \n",
        "    def compensate_loss_jump(pair, total_loss, source_node_id, jump_factor):\n",
        "        nid, (rank, loss) = pair\n",
        "        if nid == source_node_id:\n",
        "            rank = (rank + total_loss) * (1 - jump_factor) + jump_factor\n",
        "            return nid, (rank, 0)\n",
        "        return nid, (rank * (1 - jump_factor), 0)\n",
        "    \n",
        "    text_RDD = sc.textFile('p2p-Gnutella08-adj.txt')\n",
        "    graph_RDD = text_RDD.map(lambda line : construct_graph(line)).cache()\n",
        "    curr_rank_RDD = text_RDD.map(lambda line : initialize_rank(line, source_node_id))\n",
        "    num_nodes = graph_RDD.count()\n",
        "    \n",
        "    while True: \n",
        "        prev_rank_RDD = curr_rank_RDD.cache()\n",
        "        curr_rank_RDD = graph_RDD.leftOuterJoin(prev_rank_RDD) \\\n",
        "                            .flatMap(lambda pair : spread(pair)) \\\n",
        "                            .reduceByKey(lambda x, y : (x[0] + y[0], x[1] + y[1]))\n",
        "        total_loss = curr_rank_RDD.aggregate(0,\n",
        "                                            (lambda acc, e : acc + e[1][1]),\n",
        "                                            (lambda acc1, acc2 : acc1 + acc2))\n",
        "        curr_rank_RDD = curr_rank_RDD.map(lambda e : compensate_loss_jump(e, total_loss, source_node_id, jump_factor))\n",
        "        \n",
        "        max_change = curr_rank_RDD.join(prev_rank_RDD) \\\n",
        "                                  .aggregate(0,\n",
        "                                            (lambda acc, e : max(acc, abs(e[1][0][0] - e[1][1][0]))),\n",
        "                                            (lambda acc1, acc2 : max(acc1, acc2)))\n",
        "        if max_change < (0.5 / num_nodes): break\n",
        "        \n",
        "    top_10_rank = curr_rank_RDD.map(lambda e : (e[1][0], e[0])) \\\n",
        "                               .sortByKey(ascending=False) \\\n",
        "                               .map(lambda e : (e[1], e[0])) \\\n",
        "                               .take(10)\n",
        "    return top_10_rank"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(28, 0.5384424922495424),\n",
              " (1517, 0.12026767556203095),\n",
              " (1516, 0.12026368682524335),\n",
              " (846, 0.12026334287813738),\n",
              " (852, 0.008058825545098519),\n",
              " (152, 0.008057101055510536),\n",
              " (847, 0.008056064295146433),\n",
              " (850, 0.008056047096075143),\n",
              " (849, 0.008056044737059032),\n",
              " (848, 0.008056043598862838)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 309
        }
      ]
    }
  ]
}